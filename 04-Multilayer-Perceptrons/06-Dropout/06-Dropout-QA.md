# Question And Answer



11. dropout 随机置 0 对求梯度和反向传播的影响是什么？

    我的理解：从生物学的角度上来看，当使用 dropout 进行神经网络训练的时候，在正向传播的这个过程中，有一些神经元节点没有进行信息的表达。

    老师的讲解：如果使用了 dropout 那么在反向传播梯度的时候，在当前这一轮的模型训练过程中 dropout 置零的那个神经元节点就不会向后反向传播梯度了，这也就意味着有一些权重变量 w 是不会在本次训练中被更新的。

    综上：既然有一些权重变量不会被更新，这也就说明了，在当前这一轮的训练过程中其实是训练了一个子神经网络。使用这种方式来训练神经网络，每次训练一个子网络。

12. 丢弃发的丢弃依据是什么？如果丢弃不合理对输出的结果影响会很大？

    我的理解：从生物学的角度来看，神经元是不是表达，本来就是带有一定的随机性，这个世界或许就是随机的？

    老师的讲解：dropout 是一个超参数，是可以调的，合不合理就是最后的效果好不好，调节超参数，找到一个尽量最好的超参数的值。



13. dropout 随机丢弃，如何保证结果的正确性和可重复性？

正确性：机器学习没有正确性，只有效果好不好，在代码出现问题的时候也不容易排查。在实际上而言，没有正确性可言 ~ 

可重复性：就是每一次跑都是不一样的，但是可以设置随机种子来实现重复性。

随机数的来源：1 权重初始值 2 随机种子 3 cudnn 的计算由于计算机的体系结构的问题，计算也是不确定的



14. 丢弃法是在训练中把神经元丢弃后训练，在预测时网络中的神经元没有丢弃，是这样吗？

    是的。在训练的过程中会把数据分成一个一个的 batch_size 来训练，每一轮的 batch_size 的训练过程中被丢弃的神经元都不一样，所以在每一轮中不被更新的权重变量都是不一样的，也就是每一次训练的过程中训练的子网络都是不一样的。

    使用 dropout 像是人为的为数据添加了噪音，这样以便适应纷繁复杂的输入样本。但是从最终的效果来看这其实就是更像正则化的过程。

    

15. 丢弃法是每次迭代一次，随机丢弃一次吗？

    每次迭代一次就是前向运算一次 batch_size 数量个的数据样本，但是这些数据样本是同时被前向计算的，因为矩阵运算可以批量化数据处理的过程。

    

16. 请问老师，在使用 BN 的时候，还有必要使用 dropout 吗？

    BN 是用在 CNN 卷积网络中的，BN 用在卷积层，而 dropout 用在全连接层。

    

17. dropout 会不会让训练的 loss 曲线方差变大，不够平滑？

    有可能平滑，也有可能平滑，但是曲线平滑不平滑不重要，但是训练的后期如果不平滑则说明训练没有收敛，所以最后的训练阶段曲线平滑很重要。

18. 老师，请问可以再解释一下为什么 推理中的 dropout 是直接返回输入吗？

    在推理的过程中不使用 dropout，但是引入 dropout 就引入了随机性，可是推理的过程只关心某一个样本，随机性可能导致结果不同，所以多预测几次来取一个平均作为推理的最终结果。但是训练的过程中开启 dropout 并没有影响，因为训练的样本集很大而且训练的次数很大，在整个系统中引入一点随机性并不会有太大的影响。

19. 请问沐神，dropout 函数返回值的表达式 return X*mask/(1.0-p) 没有被丢弃的输入部分的值会因为表达式分母 (1-p) 的存在而改变，而训练数据集的标签还是原来的值，这怎么理解？

    标签也可以改，可以用来正则化，可以用别的算法。dropout 是最早的在神经网络中引入随机性来正则化的方法，但是 dropout 还可以用在很多地方。在 dropout 后出现了很多的算法，就是把这个把那个改成 0 或者怎么样，这都是很常见的了。

20. 请问沐神，训练时使用 dropout，而推理的时候不用。那会不会导致推理时输出结果翻倍了？比如 dropout=0.5，推理时输出结果是训练时 2 个神经自网络叠加而翻倍？

    我的理解：在推理（正向传播）的时候不适用 dropout，不过这其实也就等价于 dropout=0 的情况，就是所有的神经元节点都不丢弃，或者说是丢弃的概率为 0 

    老师的讲解a：在训练的时候 dropout=0.5 就相当于丢弃了一般的神经网络，但是剩下了那一半的神经网络于此同时也除以了 1-0.5=0.5，这也就相当于乘以了一个 2，所以在训练的时候方差没有发生变化，数学期望 E 没有发生变化，所以说没有导致翻倍的情况发生。

21. 老师，dropout 每次随机选择子网络，多做几次预测，最后做平均的方法是不是类似于随机森林多决策树做投票的这种思想？

    但是实际上这样的做法没有什么太好的效果，根据试验结果来说 dropout 更像是正则项。因为神经网络的算法没法被证明，所以只能是通过实验结果来说明问题。

22. 在解决过拟合问题上，dropout 和 regularization 的主要区别是什么？

    dropout 是一种解决过拟合的方法，（weight decay）权重衰退 L2 范式也是一种解决过拟合的方法，两种方法都可以一起使用。

24. dropout 已经被 google 申请专利，产品开发有替代方法吗？

    无所谓了 ~ 

25. 请问老师：dropout 丢弃的是前一层还是后一层？

    丢弃了前一层的输出，后一层的输入，可以把 dropout 理解为一个

26. dropout 和 weight decay（权重衰退）都属于正则，为何 dropout 效果更好，现在更常用呢？

    dropout 很好调参

27. 改标签是一种 mask 吗？

28. 在同样的 lr 下，dropout 的介入会不会造成参数收敛更慢，需要比没有 dropout 的情况下适当调大 lr 吗？

    暂时没有听说过这种情况