{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 矩阵的 sum 运算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[5., 7., 9.]]),\n",
       " array([[ 6.],\n",
       "        [15.]]))"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from IPython import display\n",
    "from d2l import torch as d2l\n",
    "\n",
    "X = np.array([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n",
    "# 0 ==> 行方向 ==> 压缩成一行\n",
    "# 1 ==> 列方向 ==> 压缩成一列\n",
    "X.sum(0, keepdims=True), X.sum(1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[5., 7., 9.]]),\n",
       " tensor([[ 6.],\n",
       "         [15.]]))"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.tensor([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n",
    "X.sum(0, keepdim=True), X.sum(1, keepdim=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 加载数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256 # 指明每一次数据的迭代都会抛出 batch_size 个样本数据\n",
    "train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义 Softmax 函数\n",
    "\n",
    "$$\n",
    "\\operatorname{softmax}(\\mathbf{X})_{i j}=\\frac{\\exp \\left(\\mathbf{X}_{i j}\\right)}{\\sum_{k} \\exp \\left(\\mathbf{X}_{i k}\\right)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(X):\n",
    "    \"\"\"\n",
    "    softmax 函数的输入是一个矩阵\n",
    "    这里的 X 是经过神经网络的线性变换后的输出\n",
    "    而不是单纯的样本数据；\n",
    "    softmax 回归是一个分类器，它有多个输出端\n",
    "    X 矩阵的每一行都是一个样本数据经过神经网络后的分类个数个的输出值\n",
    "    之所以是一个矩阵，这是为了兼容一次性输入多个样本后的多个样本的输出\n",
    "    \"\"\"\n",
    "    X_exp = np.exp(X)\n",
    "    partition = X_exp.sum(1, keepdims=True)\n",
    "    return X_exp / partition # 这里用到了广播，partition 横向扩展成矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.09003057, 0.24472847, 0.66524096],\n",
       "        [0.09003057, 0.24472847, 0.66524096]]),\n",
       " array([[1.],\n",
       "        [1.]]))"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n",
    "softmax(X), softmax(X).sum(1, keepdims=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 初始化参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_inputs = 784\n",
    "num_outputs = 10\n",
    "\n",
    "W = torch.normal(0, 0.01, size=(num_inputs, num_outputs), requires_grad=True)\n",
    "b = torch.zeros(num_outputs, requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义网络模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "def net(X):\n",
    "    \"\"\"\n",
    "    X.reshape 和 W 点乘后记为矩阵 C\n",
    "    C 的每一行都是一个样本数据经过 num_outpus 个神经元线性变化但没有加偏置项的输出\n",
    "    所以偏置项应该依次加在 C 矩阵每一行上的每一个元素，这样：b 是一个行向量是没有问题的\n",
    "    广播机制后，复制 行向量b 纵向扩展为一个矩阵\n",
    "    \"\"\"\n",
    "    return softmax(torch.matmul(X.reshape((-1, W.shape[0])), W) + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1000, 0.5000])"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.tensor([0, 2])\n",
    "y_hat = torch.tensor([[0.1, 0.3, 0.6], [0.3, 0.2, 0.5]])\n",
    "# A[[a, b], [c, d]] \n",
    "# ==> 选取了 A 矩阵的 ab 位置的元素 和 cd 位置的元素\n",
    "# Aab 和 Acd 两个元素 \n",
    "\n",
    "# 选取 0 号样本的  0 号类别的预测概率\n",
    "# 选取 1 号样本的 2 号类别的预测概率\n",
    "y_hat[[0, 1], y]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.3026, 0.6931])"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def cross_entropy(y_hat, y):\n",
    "    \"\"\"\n",
    "    y_hat 是一个矩阵，该矩阵的第 i 号行为 i 号样本的预测概率分布向量\n",
    "    y_prob 是 label 的 one-hot 编码\n",
    "    y_prob[i] = [0, 0, ... , 1, 0, 0] 只有第 j 号是 1，代表它是第 j 号类别\n",
    "    对于样本 i 而言，-log(y_hat[i])*y_prob[i]\n",
    "    由于 y_prob[i] 向量中只有第 j 号元素是 1，其他元素都为 0\n",
    "    那么 y_prob[i] 作为系数而言为 1 的那一项的系数可以不写，而其他所有的项的系数都为 0\n",
    "    所以 i 号样本的交叉熵计算过程中，其实只有一项被保留下来了，其他项都由于 0 这个系数被消除了\n",
    "    则，属于 j 号类别的 i 号样本的交叉熵为 : -log(y_hat[i][j])*y_prob[i][j]=-log(y_hat[i][j])\n",
    "    \"\"\"\n",
    "    return - torch.log(y_hat[range(len(y_hat)), y])\n",
    "\n",
    "# 得到的返回值是一个向量 ==> 所有样本的交叉熵损失值\n",
    "cross_entropy(y_hat, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### argmax 函数使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.tensor([\n",
    "    [1, 2, 3],\n",
    "    [6, 5, 4],\n",
    "    [7, 9, 8]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2],\n",
       "        [0],\n",
       "        [1]])"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 在列方向上压缩成一列\n",
    "# 但是返回的 res 的值\n",
    "# 并不是 X 矩阵中的元素\n",
    "# 而是 X 矩阵中每一列的最大值的下标\n",
    "res = X.argmax(axis=1, keepdim=True)\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 分类准确度函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_hat, y):  #@save\n",
    "    \"\"\"计算预测正确的数量\"\"\"\n",
    "    if len(y_hat.shape) > 1 and y_hat.shape[1] > 1:\n",
    "        # 把概率分布 y_hat 压缩为与 y 类似的标签\n",
    "        # y_hat 中 i 行上的向量中最大的概率的下标\n",
    "        # 就是样本 i 的预测类别的离散数值表示\n",
    "        y_hat = y_hat.argmax(axis=1)\n",
    "    cmp = y_hat.type(y.dtype) == y\n",
    "    return float(cmp.type(y.dtype).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 举个例子 ==> 前边已经定义过 y_hat 和 y 了\n",
    "# y = torch.tensor([1, 0])\n",
    "# y_hat = softmax(torch.tensor([\n",
    "#     [6, 9],\n",
    "#     [16, 10]\n",
    "# ]).reshape(2, 2))\n",
    "# y, y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2],\n",
       "        [2]])"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_hat = y_hat.argmax(axis=1, keepdim=True)\n",
    "label_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0, 0.5)"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使用 accuracy 函数试验\n",
    "# accuracy 计算的是预测正确的数量而不是比例\n",
    "accuracy(y_hat, y), accuracy(y_hat, y) / len(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义 Accumulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用 Accumulator 是因为\n",
    "# 在预测数据集过大的时候，不能够一次性加入内存的时候\n",
    "# 预测正确的数量 不能够一次性统计完成\n",
    "# 所以需要每预测以小批量的测试样本后就统计一次预测正确的数值类加上去\n",
    "# 最后完成了所有测试数据集的样本后，\n",
    "# 得到 ==> 分类准确度 = 预测正确的数量 / 总数\n",
    "\n",
    "class Accumulator:  #@save\n",
    "    \"\"\"在n个变量上累加\"\"\"\n",
    "    def __init__(self, n):\n",
    "        self.data = [0.0] * n\n",
    "\n",
    "    def add(self, *args):\n",
    "        self.data = [a + float(b) for a, b in zip(self.data, args)]\n",
    "\n",
    "    def reset(self):\n",
    "        self.data = [0.0] * len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy(net, data_iter):  #@save\n",
    "    \"\"\"计算在指定数据集上模型的精度\"\"\"\n",
    "    # 应该是对于 Moudle 的类型需要打开评估模式\n",
    "    # 才能够这样使用么 ~ 疑问句 ~  \n",
    "    if isinstance(net, torch.nn.Module):\n",
    "        net.eval()  # 将模型设置为评估模式\n",
    "    metric = Accumulator(2)  # 正确预测数、预测总数\n",
    "    with torch.no_grad():\n",
    "        for X, y in data_iter:\n",
    "            # numel ==> number of elements 元素个数\n",
    "            metric.add(accuracy(net(X), y), y.numel())\n",
    "    return metric[0] / metric[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.1127, 256)"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_accuracy(net, test_iter), len(next(iter(test_iter))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5ec60190fd82d19059a2c75283b4d0c60a6036d5032fb7c4db6e5c318b4fdfbb"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
