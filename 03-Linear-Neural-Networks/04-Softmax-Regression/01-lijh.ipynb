{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1+np.exp(-x))\n",
    "\n",
    "def d_sidmoid(x):\n",
    "    return sigmoid(x)*(1-sigmoid(x))\n",
    "\n",
    "# sigmoid 函数的导数 ==> sidmoid(x)*(1-sigmoid(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = 0.15\n",
    "w2 = 0.2\n",
    "w3 = 0.25\n",
    "w4 = 0.3\n",
    "b1 = 0.35\n",
    "\n",
    "w5 = 0.4\n",
    "w6 = 0.45\n",
    "w7 = 0.5\n",
    "w8 = 0.55\n",
    "b2 = 0.6\n",
    "\n",
    "i1 = 0.05\n",
    "i2 = 0.1\n",
    "\n",
    "true_o1 = 0.01\n",
    "true_o2 = 0.99"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 输入层 ==> 隐含层的输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.3775, 0.39249999999999996)"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net_h1 = i1*w1 + i2*w2 + b1 # 线性中间值\n",
    "net_h2 = w3*i1 + w4*i2 + b1 # 线性中间值\n",
    "net_h1, net_h2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5932699921071872, 0.596884378259767)"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_h1 = sigmoid(net_h1)\n",
    "out_h2 = sigmoid(net_h2)\n",
    "out_h1, out_h2 # 说明 h1 和 h2 使用的偏置项是同一个"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 隐含层 ==> 输出层的输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.10590596705977, 1.2249214040964653)"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net_o1 = w5*out_h1 + w6*out_h2 + b2\n",
    "net_o2 = w7*out_h1 + w8*out_h2 + b2\n",
    "net_o1, net_o2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7513650695523157, 0.7729284653214625)"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_o1 = sigmoid(net_o1)\n",
    "out_o2 = sigmoid(net_o2)\n",
    "out_o1, out_o2 # 说明 o1 和 o2 都是用了同一个偏置项 b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def square_loss(o, true_o):\n",
    "    return np.square(o - true_o)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 计算总误差"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.274811083176155, 0.023560025583847746, 0.2983711087600027)"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_o1 = square_loss(out_o1, true_o1)\n",
    "loss_o2 = square_loss(out_o2, true_o2)\n",
    "total_loss = loss_o1/2 + loss_o2/2\n",
    "loss_o1/2, loss_o2/2, total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7413650695523157"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1 total_loss ==> 对 out_o1 的偏导数\n",
    "total_loss_partial_out_o1 = out_o1 - true_o1\n",
    "total_loss_partial_out_o1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.18681560180895948"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2 out_o1 ==> 对 net_o1 的偏导数\n",
    "out_o1_partial_net_o1 = out_o1*(1-out_o1)\n",
    "out_o1_partial_net_o1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5932699921071872"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3 net_o1 ==> 对 w5 求导\n",
    "net_o1_partial_w5 = out_h1\n",
    "net_o1_partial_w5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.08216704056423078"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# total_loss ==> 对 w5 求导 \n",
    "total_loss_partial_w5 = total_loss_partial_out_o1 * out_o1_partial_net_o1 * net_o1_partial_w5\n",
    "total_loss_partial_w5 # 这样就可以更新 w5 的权重了\n",
    "# 所以到这里应该明白，每一个权重的导数值都是从尾向前进行的\n",
    "# 每一个权重的导数值都要从尾部开始，而不可能只是从中间某个位置开始向前"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 计算 w1 的偏导数\n",
    "原文链接：https://www.cnblogs.com/charlotte77/p/5629865.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 依然是使用链式求导法则一次向前\n",
    "# 但是在求 w1 这个参数的篇导数值的时候\n",
    "# 其他所有的 wi 都视为常数！这也正是求偏导的常用技巧\n",
    "# 但是其他的 wi 其实是有值得，最后求出表达式后，\n",
    "# 只需要带入其他的 wi 的具体值就可以得到 w1 的偏导数值了\n",
    "# 一个神经元被分割为 net 和 out 两部分至关重要！\n",
    "# out(x) 函数的输入正是 net ==> 也就是 out(net(上一层的out))\n",
    "# 神经元的左边为树突，左边的权重属于右侧的这个神经元的权重，正是 net 的部分\n",
    "# -----------------------------------------------\n",
    "# 在 total_loss 到 w1 的路上，分叉要加起来求！这是值得注意的点！\n",
    "# 但是更简单的理解就是导数的定义，让 w1 有一个增量，估计此时的 total_loss\n",
    "# 从而得到 w1 的偏导数的值\n",
    "# ----------------------------------------\n",
    "# 更新参数的策略\n",
    "# pytorch 实在调用 step() 函数后才统一更新\n",
    "# 而这篇文章则是求一个参数的偏导数值后就立即更新该参数的值\n",
    "# ----------------------------------------------"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5ec60190fd82d19059a2c75283b4d0c60a6036d5032fb7c4db6e5c318b4fdfbb"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
